# Dividing and Conquering a BlackBox to a Mixture of Interpretable Models: Route, Interpret, Repeat #

### [Project Page]() | [Paper]() | [Arxiv](https://arxiv.org/pdf/2302.10289.pdf)

[Shantanu Ghosh <sup>1</sup>](https://shantanu48114860.github.io/),
[Ke Yu <sup>2</sup>](https://gatechke.github.io/),
[Forough Arabshahi <sup>3</sup>](https://forougha.github.io/),
[Kayhan Batmanghelich <sup>1</sup>](https://www.batman-lab.com/)
<br/>
<sup>1</sup> BU ECE, <sup>2</sup> Pitt ISP, <sup>3</sup> META AI <br/>
In [ICML, 2023](https://icml.cc/Conferences/2023/Dates) <br/>

## Table of Contents

1. [Objective](#objective)
2. [Environment setup](#environment-setup)
3. [Downloading data](#downloading-data)
    * [(a) Downloading vision and skin data](#a-downloading-vision-and-skin-data)
    * [(b) Downloading MIMIC-CXR](#b-downloading-mimic-cxr)
4. [Data preprocessing](#data-preprocessing)
    * [(a) Preprocessing CUB200](#a-preprocessing-cub200)
    * [(b) Preprocessing MIMIC-CXR](#b-preprocessing-mimic-cxr)
5. [Training pipeline](#training-pipleline)
    * [(a) Running MoIE](#a-running-moie)
    * [(b) Compute the performance metrics](#b-compute-the-performance-metrics)
    * [(c) Validating the concept importance](#c-validating-the-concept-importance)
6. [Checkpoints](#checkpoints)
7. [How to Cite](#how-to-cite)

## Objective

In this paper, we aim to blur the dichotomy of explaining a Blackbox post-hoc and building inherently interpretable by
design models. Beginning with a Blackbox, we iteratively *carve out* a mixture of interpretable experts (MoIE) and a *
residual network*. Each interpretable model specializes in a subset of samples and explains them using First Order
Logic (FOL). We route the remaining samples through a flexible residual. We repeat the method on the residual network
until all the interpretable models explain the desired proportion of data. Thus, illustration of our method is
summarized below:

<img src='images/method.gif'><br/>

Refer below for the example of the explanations generated by MoIE:
<img src='images/cub-mimic.gif'><br/>

## Environment setup

```bash
conda env create --name python_3_7_rtx_6000 -f environment.yml
conda activate python_3_7_rtx_6000
```

## Downloading data

### (a) Downloading vision and skin data

| Dataset   | Description                        | URL                                                                                 |
|-----------|------------------------------------|-------------------------------------------------------------------------------------|
| CUB-200   | Bird Classification dataset        | [CUB-200 Official](https://www.vision.caltech.edu/datasets/cub_200_2011/)           |
| Derm7pt   | Dermatology Concepts Dataset       | [Get access here](https://derm.cs.sfu.ca/Welcome.html)                              |
| HAM10k    | Skin lesion classification dataset | [Kaggle Link](https://www.kaggle.com/kmader/skin-cancer-mnist-ham10000)             |
| SIIM_ISIC | Skin Melanoma classification       | [SIIM-ISIC Kaggle](https://www.kaggle.com/c/siim-isic-melanoma-classification/data) |
| Awa2      | Animals with Attributes2           | [Awa2 official](https://cvml.ista.ac.at/AwA2/)                                      |

### (b) Downloading MIMIC-CXR

- [MIMIC-CXR](https://physionet.org/content/mimic-cxr-jpg/2.0.0/)
- [RadGraph](https://physionet.org/content/radgraph/1.0.0/)
- [NVIDIA Annotation](https://github.com/leotam/MIMIC-CXR-annotations)
- [Chest ImaGenome](https://physionet.org/content/chest-imagenome/1.0.0/)

For more details please follow the [AGXNet Repository](https://github.com/batmanlab/AGXNet).

## Data preprocessing

### (a) Preprocessing CUB200

To get the CUB200 metadata and dataset splits
follow [Logic Explained network](https://github.com/pietrobarbiero/logic_explained_networks/tree/master/data).

To preprocess the concepts for CUB200, follow:

``` python
python ./src/codebase/data_preprocessing/download_cub.py
```

### (b) Preprocessing MIMIC-CXR

To preprocess MIMIC-CXR for Effusion, follow the following steps sequentially:

1. To generate itemized RadGraph examples, run:

``` python
python ./src/codebase/data_preprocessing/mimic-cxr/miccai-main/preprocessing/radgraph_itemized.py
```

2. Run `./preprocessing/radgraph_parsed.py` To parse RadGraph relations, run:

``` python
python ./src/codebase/data_preprocessing/mimic-cxr/miccai-main/preprocessing/radgraph_parsed.py
```

3. To create adjacency matrix that represents the relations between anatomical landmarks and observations mentioned in
   radiology reports, run:

``` python
python ./src/codebase/data_preprocessing/mimic-cxr/miccai-main/preprocessing/adj_matrix.py
```

Step 3 will be the concepts for training MoIE-CXR.

## Training pipeline

All the scripts for training MoIE, is included in `./src/scripts` folder for all the datasets and architectures with
comments. Follow every command sequentially of each script to train/test the Blackbox (BB), concept predictor (t),
explainers (g) and residuals (r). Refer to the following sections for details of each of the scripts.

### (a) Running MoIE

| Script name                      | Description                                                            | Comment                                                                                                   |
|----------------------------------|------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------|
| `./src/script/cub_resnet.sh`     | Script for CUB200 dataset with Resnet101 as the Blackbox (BB)          | Included train/test script for the Blackbox (BB), concept predictor (t), explainers (g) and residuals (r) |
| `./src/script/cub_vit.sh`        | Script for CUB200 dataset with Vision Transformer as the Blackbox (BB) | Included train/test script for the Blackbox (BB), concept predictor (t), explainers (g) and residuals (r) |
| `./src/script/awa2_resnet.sh`    | Script for Awa2 dataset with Resnet101 as the Blackbox (BB)            | Included train/test script for the Blackbox (BB), concept predictor (t), explainers (g) and residuals (r) |
| `./src/script/awa2_vit.sh`       | Script for Awa2 dataset with Vision Transformer as the Blackbox (BB)   | Included train/test script for the Blackbox (BB), concept predictor (t), explainers (g) and residuals (r) |
| `./src/script/ham10k.sh`         | Script for HAM10k dataset with Inception_v3 as the Blackbox (BB)       | Included train/test script for the Blackbox (BB), concept predictor (t), explainers (g) and residuals (r) |-------------------------------|---------------------------------------------------------------|--------------------------------------------------------------------------------------------------------|
| `./src/script/SIIM-ISIC.sh`      | Script for SIIM-ISIC dataset with Inception_v3 as the Blackbox (BB)    | Included train/test script for the Blackbox (BB), concept predictor (t), explainers (g) and residuals (r) |
| `./src/script/mimic_effusion.sh` | Script for MIMIC-CXR dataset with Densenet121 as the Blackbox (BB)     | Included train/test script for the Blackbox (BB), concept predictor (t), explainers (g) and residuals (r) |  

For reference, check the following repositories for SOTA Blackboxes and concepts:

* [ResNet-101 on CUB-200](https://github.com/zhangyongshun/resnet_finetune_cub)
* [VIT-B_16 on CUB-200](https://github.com/TACJu/TransFG)
* [Models and concepts for HAM10k and ISIC](https://github.com/mertyg/post-hoc-cbm)

## License & copyright

Licensed under the [MIT License](LICENSE)

Copyright (c) [Batman Lab](https://www.batman-lab.com/), 2023
