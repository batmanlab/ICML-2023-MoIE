 
Model explainability is essential in high-stakes applications of AI, such as healthcare. While BlackBox models (\eg Deep Learning) offer flexibility and modular design, post hoc explanation is prone to confirmation bias~\cite{wan2022explainability}, lack of fidelity to the original model~\cite{adebayo2018sanity}, and insufficient mechanistic explanation of the decision-making process~\cite{rudin2019stop}. Interpretable-by-design models do not suffer from those issues but tend to be less flexible than Blackbox models and demand substantial expertise to design and fine-tune. Using post hoc explanation or adopting an interpretable model is a mutually exclusive decision to be made at the initial phase of AI model design. This paper aims to blur the line on that dichotomous model design. 

The literature on post hoc explainable AI is extensive. The methods such as model attribution (\eg Saliency Map~\cite{simonyan2013deep, selvaraju2017grad}), counterfactual approach ~\cite{abid2021meaningfully, singla2019explanation}, and distillation methods~\cite{alharbi2021learning, cheng2020explaining} are examples of post hoc explainability approaches. Those methods either identify important features of input that contribute the most to the network's output~\cite{shrikumar2016not}, generate perturbation to the input that flips the network's output~\cite{samek2016evaluating}, \cite{montavon2018methods}, or estimate simpler functions that locally approximate the network output. The advantage of the post hoc explainability methods is that they do not compromise the flexibility and performance of the BlackBox. However, the post hoc explainability method suffers from several undesirable significant drawbacks, such as a lack of fidelity and mechanistic explanation of the network output~\cite{rudin2019stop}. Without a mechanistic explanation, recourse to a model's undesirable behavior is unclear. Interpretable models are alternative designs to the BlackBox model that do not suffer from many of those drawbacks. 

Interpretable models also have a long history in statistics and machine learning~\cite{letham2015interpretable, breiman1984classification}. Several families of interpretable models exist, such as the rule-based approach and generalized additive models~\cite{hastie1987generalized}. Many methods focus on tabular or categorical data and less on high-dimensional structured data such as images. Interpretable models for structured data rely mostly on projecting to a lower dimensional \emph{concept} or \emph{symbolic} space that is understandable for humans~\cite{koh2020concept}. Aside from a few exceptions ~\cite{ciravegna2021logic, barbiero2022entropy}, the current State-Of-The-Art (SOTA) design does not model the interaction between concepts and symbols, hence offering limited reasoning capabilities and less robustness. Furthermore, current designs are not as flexible as the Blackbox model, which may compromise the performance of such models. 


We aim to achieve the best of both worlds: the flexibility of the BlackBox and the mechanistic explainability of the interpretable models. The general idea is that a single interpretable model may not be sufficiently powerful to explain all samples, and several interpretable models might be hidden inside the Blackbox model. We construct a hybrid neuro-symbolic model by progressively \emph{carving out} a mixture of interpretable model and a \emph{residual network}. Our design identifies a subset of samples and \emph{routes} them through the interpretable models. The remaining samples are routed through a flexible residual network. We adopt First Order Logic (FOL) as the interpretable model's backbone, which provides basic reasoning on concepts retrieved from the BlackBox model. 
FOL is a logical function
that accepts predicates (concept presence/absent) as input and returns a True/False output being a
logical expression of the predicates. The logical expression, which is a set of AND, OR, Negative,
and parenthesis, can be written in the so-called Disjunctive Normal Form (DNF). DNF is a FOL logical formula composed of a disjunction (OR) of conjunctions (AND), known as the ``sum of products''.
On the residual network, we repeat the method until the proportion of data explained by the residual network falls below a desired threshold. The experimental results across various computer vision and medical imaging datasets reveal that our method accounts for the diversity of the explanation space and has minimal impact on the Blackbox's performance. Additionally, we apply our method's explanations to detect shortcuts in computer vision and successfully eliminate the bias from the Blackbox's representation.

\paragraph{Contributions}
% \begin{figure}[t]
% \centering
% \includegraphics[width=1\textwidth]{figures/Final/Local_explanation_motivation.pdf}
% \caption{The motivation of our work. All the plots correspond to detect a skin lesion as ``Malignant". In the left, (a) plot of the weights for each concepts by the fully interpretable model; (b) explanations by the interpretable model containing the concept \textit{sex} as it has the height weight, making the explanation too generic; (c) plot of the weights for each concepts by the expert4 using our method; (d) explanations by the expert4 using our method; (e) plot of the weights for each concepts by the expert5 using our method; (f) explanations by the expert5 using our method. From (c)-(f), we can see the expert model focuses on specific concept for specific samples.}
% \label{fig:motivation} 
% \end{figure}