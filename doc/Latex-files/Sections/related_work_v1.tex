% Post hoc methods
% Saliency cam grad cam
% attribution lime shap l2x

% interpretable models
% early GAM - tabular data
% CBM no composability
% E-LENS FOL

\textbf{Post hoc explanations:} \cite{simonyan2013deep, selvaraju2017grad, smilkov2017smoothgrad} discuss the post hoc explanation method using saliency maps to explain a convolution neural network, highlighting the pixels in the input images that contributed to the network's prediction. \cite{adebayo2018sanity, kindermans2019reliability}  demonstrates that the saliency maps highlight the correct regions in the image even though the backbone's representation was arbitrarily perturbed. Additionally, in LIME~\cite{ribeiro2016should}, given a superpixel, a surrogate linear function attempts to learn the prediction of the Blackbox surrounding that superpixel. SHAP \cite{SHAP} utilizes a game-theoretic strategy called SHAPLY values to estimate the Blackbox's prediction by considering all the permutations of adding and removing a specific feature to determine its importance in the final prediction. Regarding pixel intensities, the explanations do not correspond to the high-level, human-interpretable \emph{concepts}. In this paper, we provide the post hoc explanation of the Blackbox in terms of the interpretable concepts, rather than the pixel intensities.

% \textbf{Concept-based interpretable models}: 
% In this class, the researchers try to design inherently interpretable models to eliminate the  requirement for post hoc explanations. In the literature, we find interpretable models in Generalized Additive Models (GAM)~\cite{hastie1987generalized}, or on logic formulas, as in Decision Trees~\cite{breiman1984classification} or Bayesian Rule Lists (BRL)~\cite{letham2015interpretable}. 
% % GAMs overcome the linearity assumption of linear regression by learning target categories disjointly from each feature. 
% However, most of these methods work well in categorical datasets rather than continuous data such as images. Additionally,~\cite{chen2019looks} introduces a ``case-based reasoning'' technique, known as ``ProtoPNet''. Here, the authors first dissect an image in Prototypical parts and then classifies by combining evidence from the pre-defined prototypes. This method is highly sensitive to the choice of prototypes and the distance metric used to compare with the prototype. So instead of using any prototypes, our proposed method uses human understandable concepts to build the mixture of interpretable models.
\textbf{Concept-based interpretable models:}
Recently, CBM~\cite{koh2020concept}, based on the interpretable-by-design principle, leverages the weak annotations of visual attributes to predict the high-level human-comprehensible concepts~\cite{kim2017interpretability} from images and then the class labels from the discovered concepts. A concept decoder is used in the antehoc model~\cite{sarkar2021inducing} to simultaneously reconstruct the image from the concepts to ensure the the semantics of the input image from the concepts, specifically aiming for unsupervised concept discovery. Instead of learning scalar concepts in the bottleneck, CEM~\cite{zarlenga2022concept} uses high dimensional concept embeddings to allow extra supervised learning capacity. Later,PCBM ~\cite{yuksekgonul2022post} learns the concepts from the embeddings of a trained Blackbox and uses an interpretable classifier for classification. Also, they propose the Hybrid-Post-hoc-concept-based model (PCBM-h) to replicate the performance of the Blackbox. Yet, they do not explain how the final classifier composes those concepts for prediction. An inherently interpretable ELL~\cite{barbiero2022entropy} addresses this gap by introducing an entropy-based classifier for the downstream classification and providing explanations in terms of FOL using the concepts. However, all these methods 1) employ a single interpretable model to explain a Blackbox, failing to apprehend different forms of explanations for different samples and offering generic explanations for all samples of a class 2) did not quantify if the identified concepts is faithful to the Blackbox's prediction. We introduce MoIE to address this issue, utilizing Blackbox's flexibility to identify meaningful instance-specific concepts with high concept completeness to offer a local explanation using FOL. In addition, we learn the residuals in each iteration for the samples not covered by the associated interpretable model, whereas PCBM-h trains the residual for all samples.

\textbf{Shortcut learning:}
"Shortcuts'' are defined when a predictor relies on easy-to-learn spurious correlations between the inputs and the target~\cite{geirhos2020shortcut}. Learning shortcuts can broadly be categorized either in example difficulty estimation~\cite{lalor2018understanding, hooker2019compressed, agarwal2022estimating} or monitoring training dynamics~\cite{hu2020surprising, feng2021phases, rabanser2022selective}. Unlike these methods, MoIE discovers shortcut using the high level concepts in FOL explanation of the Blackbox's prediction of a sample, routed through an interpretable model and eliminate it using MDN \cite{lu2021metadata}.