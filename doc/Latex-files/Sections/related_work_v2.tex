% Post hoc methods
% Saliency cam grad cam
% attribution lime shap l2x

% interpretable models
% early GAM - tabular data
% CBM no composability
% E-LENS FOL

\textbf{Post hoc explanations:} 
Related work in Post-hoc explanation techniques include: 1) highlighting relevant pixels in the image~\cite{simonyan2013deep, selvaraju2017grad, smilkov2017smoothgrad, sundararajan2017axiomatic, binder2016layer}, 2) developing a surrogate linear function~\cite{ribeiro2016should, yoon2019rl} to imitate the Blackbox around a local neighborhood, and 3) using the game-theoretic SHAPLY values~\cite{SHAP} to determine feature importance in the Blacbox prediction. Though post hoc explanations retain the flexibility and performance of the Blackbox, they operate in pixel space, often not corresponding to humanly interpretable \emph{concepts}~\cite{kim2017interpretability}. This restricts the use of post-hoc methods in algorithmic recourse~\cite{bordt2022post}. In this paper, we offer a post hoc explanation of the Blackbox in terms of concepts to improve interpretability and facilitate test-time interventions on the concepts.

\textbf{Concept-based interpretable models:}
Concept-based interpretable models leverage the weak annotations of visual attributes to predict the concepts from images and then the class labels from the concepts, as in Concept Bottleneck models (CBMs)~\cite{koh2020concept}. A concept decoder is used in the antehoc model~\cite{sarkar2021inducing} to simultaneously reconstruct the image from the concepts to ensure the semantics of the input image from the concepts, specifically aiming for unsupervised concept discovery. Instead of learning scalar concepts in the bottleneck, end-to-end Concept Embedding models (CEMs)~\cite{zarlenga2022concept} uses high dimensional concept embeddings to allow extra supervised learning capacity and achieves SOTA performance in the interpretable-by-design class. An inherently interpretable ELL~\cite{barbiero2022entropy} introduces an entropy-based classifier for the downstream classification and provides explanations in terms of FOL using the concepts. Recently, Posthoc Concept Bottleneck models (PCBMs) ~\cite{yuksekgonul2022post} learn the concepts from a trained Blackbox  embeddings and uses an interpretable classifier for classification. Also, they fit a residual in their hybrid variant (PCBM-h) to mimic the performance of the Blackbox. However, all these methods employ a single interpretable model to explain a Blackbox. Thus, they offer generic explanations for all the samples of a class, lowering the performance, particularly for models that are inherently interpretable. Also PCBMs do not provide any reasoning through the identified concepts. Our MoIE overcomes these issues by 1) employing multiple experts to carve instance-specific concepts from the Blackbox while preserving its flexibility and performance, 2) providing fundamental reasoning on concepts by offering FOL explanations per sample, 3) fitting residuals in each iteration to progressively cover samples in an increasing order of ``hardness''.

\textbf{Shortcut learning:}
Often, a Blackbox leverages spurious correlations, having no semantic connection to the actual task. These spurious correlations are termed as ``shortcuts''.  that the model exploits for its decision without any connection to the actual task. Related work in explainability includes LIME~\cite{ribeiro2016should}, utilized to detect spurious background as a shortcut to classify an animal. Recently interpretable model~\cite{rosenzweig2021patch}, involving local image pathces, are used as a proxy to the Blackbox to identify shortcuts. However, both of them function in pixel space, notconcept space. Also, they do not eliminate the shortcut.  Our MoIE discovers shortcuts using the high level concepts in the FOL explanation of the Blackbox's prediction and eliminates them via Meta data normalization (MDN)~\cite{lu2021metadata}.