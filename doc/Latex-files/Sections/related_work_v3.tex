
\textbf{Post hoc explanations:} 
Post hoc explanations retain the flexibility and performance of the Blackbox. The post hoc explanation has many categories, including feature attribution~\cite{simonyan2013deep, smilkov2017smoothgrad, binder2016layer} and counterfactual approaches~\cite{singla2019explanation, abid2021meaningfully}. For example, feature attribution methods associate a measure of importance to features (e.g., pixels) that is proportional to the feature's contribution to BlackBox's predicted output. Many methods were proposed to estimate the importance measure, including gradient-based methods~\cite{selvaraju2017grad, sundararajan2017axiomatic}, game-theoretic approach~\cite{SHAP}. The post hoc approaches suffer from a lack of fidelity to input~\cite{adebayo2018sanity} and ambiguity in explanation due to a lack of correspondence to human-understandable concepts. Recently, Posthoc Concept Bottleneck models (PCBMs) ~\cite{yuksekgonul2022post} learn the concepts from a trained Blackbox embedding and use an interpretable classifier for classification. Also, they fit a residual in their hybrid variant (PCBM-h) to mimic the performance of the Blackbox. We will compare against the performance of the PCBMs method. Another major shortcoming is that, due to a lack of mechanistic explanation, post hoc explanations do not provide a recourse when an undesirable property of a Blackbox is identified. Interpretable-by-design provides a remedy to those issues~\cite{rudin2019stop}.




\textbf{Concept-based interpretable models:}
Our approach falls into the category of concept-based interpretable models. 
% Our approach falls into the category of interpretable-by-design.
Such methods provide a mechanistically interpretable prediction that is a function of human-understandable concepts. The concepts are usually extracted from the activation of the middle layers of the Neural Network (bottleneck). Examples include Concept Bottleneck models (CBMs)~\cite{koh2020concept},  antehoc concept decoder~\cite{sarkar2021inducing}, and a high-dimensional Concept Embedding model (CEMs)~\cite{zarlenga2022concept} that uses high dimensional concept embeddings to allow extra supervised learning capacity and achieves SOTA performance in the interpretable-by-design class. Most concept-based interpretable models do not model the interaction between concepts and cannot be used for reasoning. An exception is E-LEN~\cite{barbiero2022entropy} which uses an entropy-based approach to derive explanations in terms of FOL using the concepts. The underlying assumption of those methods is that one interpretable function can explain the entire set of data, which can limit flexibility and consequently hurt the performance of the models. Our approach relaxes that assumption by allowing multiple interpretable functions and a residual. Each function is appropriate for a portion of the data, and a small portion of the data is allowed to be uninterpretable by the model (\ie residual). We will compare our method with CBMs, CEMs, and their E-LEN-enhanced variants. 
 


\textbf{Application in fixing the shortcut learning:}
Shortcuts are spurious features that correlate with both input and the label on the training dataset but fail to generalize in more challenging real-world scenarios. Explainable AI (X-AI) aims to identify and fix such an undesirable property. Related work in X-AI includes LIME~\cite{ribeiro2016should}, utilized to detect spurious background as a shortcut to classify an animal. Recently interpretable model~\cite{rosenzweig2021patch}, involving local image patches, are used 
as a proxy to the Blackbox to identify shortcuts. However, both methods operate in pixel space, not concept space. Also, both approaches are post hoc and do not provide a way to eliminate the shortcut 
learning problem. Our MoIE discovers shortcuts using the high-level concepts in the FOL explanation of the Blackbox's prediction and eliminates them via metadata normalization (MDN)~\cite{lu2021metadata}.

