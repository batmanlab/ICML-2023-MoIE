We compare our methods to two baselines -- 1) interpretable by design 2) posthoc concept bottlenecks (PCBMs). For interpretable by design, we employ the end to end concept embedding models (CEMs)~\cite{zarlenga2022concept} and sequential concept bottleneck models (CBMs)~\cite{koh2020concept}, consisting of two parts: 1) a concept extractor $\phi: \mathcal{X} \rightarrow \mathcal{C}$, predicting concepts from images; and 2) a classifier $g: \mathcal{C} \rightarrow \mathcal{Y}$, predicting labels from the concepts. Convolution-based $\phi$ includes all layers till the last convolution block. VIT-based $\phi$ consists of the transformer encoder block (excluding the classification head). For PCBMs, we employ the similar strategy described in~\cite{yuksekgonul2022post} to classify using the extracted the concepts from image embeddings of a trained blackbox. Furthermore, we use the identical $g$ of MOIE (as in Appendix \ref{app:g}) replacing the label predictors of CBM and PCBM to compare our FOL explanations with the baselines. Henceforth, we refer them as CBM + ELL and PCBM + ELL. As~\cite{barbiero2022entropy} trains the concept and label predictors sequentially, we also perform the same for the CBM + ELL and PCBM + ELL baselines. For HAM10000 dataset, we extract the concepts from Derm7pt dataset using the pretrained embeddings of the blackbox as per~\cite{yuksekgonul2022post}. Thus, we can not use interpretable by design baselines for HAM10000 and SIIM-ISIC due to the unavailability of concept annotations in the dataset.