\paragraph{CUB-200}
The Caltech-UCSD Birds-200-2011 (\cite{wah2011caltech}) is a fine-grained classification dataset comprising 11788 images and 312 noisy visual concepts. The aim is to classify the correct bird species from 200 possible classes. We adopted the strategy discussed in~\cite{barbiero2022entropy} to extract 108 denoised visual concepts. Also, we utilize training/validation splits shared in \cite{barbiero2022entropy}. Finally, we use the state-of-the-art classification models Resnet-101 (\cite{he2016deep}) and Vision-Transformer (VIT) (\cite{wang2021feature}) as the blackboxes $f^0$. 

% For the baseline, we train the convolution layers and the transformer encoder layers of Resnet-101 and the vision transformer respectively to learn the mapping $\mathcal{X} \rightarrow \mathcal{C}$ from scratch. Finally we used the concepts extracted in the previous 

\paragraph{Animals with attributes2 (Awa2)}
 AwA2 dataset \cite{xian2018zero} consists of 37322 images of total 50 animals classes with 85 numeric attribute. We use the state-of-the-art classification models Resnet-101 (\cite{he2016deep}) and Vision-Transformer (VIT) (\cite{wang2021feature}) as the blackboxes $f^0$.

\paragraph{HAM10000}
HAM10000 (\cite{tschandl2018ham10000}) is a classification dataset aiming to classify a skin lesion benign or malignant. Following \cite{daneshjou2021disparities}, we use Inception \cite{szegedy2015going} model, trained on this dataset as the blackbox $f^0$. We follow the strategy in \cite{lucieri2020interpretability} to extract the 8 concepts from the Derm7pt (\cite{kawahara2018seven}) dataset.

\paragraph{SIIM-ISIC}
To test a real-world transfer learning use case, we evaluate the
model trained on HAM10000 on a subset of the SIIM-ISIC\cite{rotemberg2021patient}) Melanoma Classification dataset. We use
the same concepts described in the HAM10000 dataset.

\paragraph{MIMIC-CXR} We use  220,763 frontal images from the MIMIC-CXR dataset \cite{12_johnsonmimic} aiming to classify effusion. We obtain the anatomical and observation concepts from the RadGraph annotations in RadGraphâ€™s inference dataset (\cite{10_jain2021radgraph}), automatically generated by DYGIE++ (\cite{23_wadden-etal-2019-entity}). We use the test-train-validation splits from \cite{yu2022anatomy} and Densenet121 \cite{huang2017densely} as the blackbox $f^0$. 




